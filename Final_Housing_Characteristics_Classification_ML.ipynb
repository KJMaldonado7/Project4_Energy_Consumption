{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x1bb3c7f8390>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to SQLAlchemy\n",
    "engine = create_engine(\"postgresql://postgres:%s@localhost/energy_consumption\" % quote_plus(\"postgres\"))\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doeid</th>\n",
       "      <th>typehuq</th>\n",
       "      <th>yearmaderange</th>\n",
       "      <th>totrooms</th>\n",
       "      <th>walltype</th>\n",
       "      <th>rooftype</th>\n",
       "      <th>adqinsul</th>\n",
       "      <th>numfrig</th>\n",
       "      <th>equipm</th>\n",
       "      <th>acequipm_pub</th>\n",
       "      <th>totsqft_en</th>\n",
       "      <th>totalbtu</th>\n",
       "      <th>totaldol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>144647.71</td>\n",
       "      <td>2656.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>590</td>\n",
       "      <td>28034.61</td>\n",
       "      <td>975.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>30749.71</td>\n",
       "      <td>522.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>86765.19</td>\n",
       "      <td>2061.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>59126.93</td>\n",
       "      <td>1463.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doeid  typehuq  yearmaderange  totrooms  walltype  rooftype  adqinsul  \\\n",
       "0  100001        2              4         8         1         5         2   \n",
       "1  100002        5              5         3         1        -2         2   \n",
       "2  100003        5              3         4         1        -2         2   \n",
       "3  100004        2              5         9         3         5         2   \n",
       "4  100005        5              3         3         7        -2         2   \n",
       "\n",
       "   numfrig  equipm  acequipm_pub  totsqft_en   totalbtu  totaldol  \n",
       "0        2       3             1        2100  144647.71   2656.89  \n",
       "1        1       3             1         590   28034.61    975.00  \n",
       "2        0       2             1         900   30749.71    522.65  \n",
       "3        2       3             1        2100   86765.19   2061.77  \n",
       "4        2       3             1         800   59126.93   1463.04  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_housing = pd.read_sql_table('housing_characteristics', engine)\n",
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typehuq</th>\n",
       "      <th>yearmaderange</th>\n",
       "      <th>totrooms</th>\n",
       "      <th>walltype</th>\n",
       "      <th>rooftype</th>\n",
       "      <th>adqinsul</th>\n",
       "      <th>numfrig</th>\n",
       "      <th>equipm</th>\n",
       "      <th>acequipm_pub</th>\n",
       "      <th>totsqft_en</th>\n",
       "      <th>totalbtu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>144647.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>590</td>\n",
       "      <td>28034.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>30749.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>86765.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>59126.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4520</td>\n",
       "      <td>85400.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>131875.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>900</td>\n",
       "      <td>41446.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>750</td>\n",
       "      <td>14512.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>12393.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1630</td>\n",
       "      <td>58496.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1540</td>\n",
       "      <td>73867.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>83979.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1450</td>\n",
       "      <td>30595.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1650</td>\n",
       "      <td>98033.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1350</td>\n",
       "      <td>39937.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1910</td>\n",
       "      <td>112775.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2650</td>\n",
       "      <td>125582.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>51945.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>53803.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    typehuq  yearmaderange  totrooms  walltype  rooftype  adqinsul  numfrig  \\\n",
       "0         2              4         8         1         5         2        2   \n",
       "1         5              5         3         1        -2         2        1   \n",
       "2         5              3         4         1        -2         2        0   \n",
       "3         2              5         9         3         5         2        2   \n",
       "4         5              3         3         7        -2         2        2   \n",
       "5         2              6         8         1         5         1        2   \n",
       "6         2              2         5         1         5         3        1   \n",
       "7         5              7         4         3        -2         2        1   \n",
       "8         5              7         3         7        -2         2        1   \n",
       "9         5              5         4         4        -2         2        1   \n",
       "10        2              4         7         4         1         2        2   \n",
       "11        2              8         9         2         2         2        1   \n",
       "12        2              4         8         1         5         2        1   \n",
       "13        2              2         6         1         5         2        1   \n",
       "14        2              2         7         2         3         3        1   \n",
       "15        3              3         5         4         5         1        1   \n",
       "16        2              5         6         2         5         1        2   \n",
       "17        2              6         6         4         5         2        1   \n",
       "18        3              4         4         6         5         2        1   \n",
       "19        2              3         4         2         3         1        2   \n",
       "\n",
       "    equipm  acequipm_pub  totsqft_en   totalbtu  \n",
       "0        3             1        2100  144647.71  \n",
       "1        3             1         590   28034.61  \n",
       "2        2             1         900   30749.71  \n",
       "3        3             1        2100   86765.19  \n",
       "4        3             1         800   59126.93  \n",
       "5        3             1        4520   85400.64  \n",
       "6        3             1        2100  131875.03  \n",
       "7        4            -2         900   41446.59  \n",
       "8        5             4         750   14512.02  \n",
       "9        4             1         760   12393.76  \n",
       "10       3             1        1630   58496.25  \n",
       "11       7             1        1540   73867.11  \n",
       "12       3             1        2000   83979.36  \n",
       "13       2             4        1450   30595.98  \n",
       "14       3             4        1650   98033.63  \n",
       "15       5             1        1350   39937.69  \n",
       "16       2             6        1910  112775.20  \n",
       "17       2             6        2650  125582.35  \n",
       "18       3             1        1980   51945.31  \n",
       "19       4             1        1300   53803.14  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read housing_characteristics csv file in the Table_CSVs folder\n",
    "#df_housing = pd.read_csv(\"Table_CSVs/housing_characteristics.csv\")\n",
    "\n",
    "#drop missing values\n",
    "df_housing.dropna(inplace=True)\n",
    "\n",
    "#drop unnecessary \n",
    "df_housing.drop(columns=[\"doeid\", 'totaldol'], inplace=True)\n",
    "\n",
    "#review dataframe\n",
    "df_housing.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of samples in each bin:\n",
      "btubin\n",
      "0    6166\n",
      "2    6165\n",
      "1    6165\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of bins\n",
    "num_bins = 3\n",
    "\n",
    "# Perform equal-frequency binning\n",
    "df_housing['btubin'] = pd.qcut(df_housing['totalbtu'], q=num_bins, labels=False)\n",
    "\n",
    "#Check the balance of bins\n",
    "bin_counts = df_housing['btubin'].value_counts()\n",
    "\n",
    "# Plot the distribution of bins\n",
    "plt.figure(figsize=(8, 6))\n",
    "bin_counts.plot(kind='bar', rot=0)\n",
    "plt.title('Distribution of Bins')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Display the counts of samples in each bin\n",
    "print(\"Counts of samples in each bin:\")\n",
    "print(bin_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typehuq</th>\n",
       "      <th>yearmaderange</th>\n",
       "      <th>totrooms</th>\n",
       "      <th>walltype</th>\n",
       "      <th>rooftype</th>\n",
       "      <th>adqinsul</th>\n",
       "      <th>numfrig</th>\n",
       "      <th>equipm</th>\n",
       "      <th>acequipm_pub</th>\n",
       "      <th>totsqft_en</th>\n",
       "      <th>totalbtu</th>\n",
       "      <th>btubin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>144647.71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>590</td>\n",
       "      <td>28034.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>30749.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>86765.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>59126.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4520</td>\n",
       "      <td>85400.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2100</td>\n",
       "      <td>131875.03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>900</td>\n",
       "      <td>41446.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>750</td>\n",
       "      <td>14512.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>12393.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1630</td>\n",
       "      <td>58496.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1540</td>\n",
       "      <td>73867.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>83979.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1450</td>\n",
       "      <td>30595.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1650</td>\n",
       "      <td>98033.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1350</td>\n",
       "      <td>39937.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1910</td>\n",
       "      <td>112775.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2650</td>\n",
       "      <td>125582.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>51945.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>53803.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    typehuq  yearmaderange  totrooms  walltype  rooftype  adqinsul  numfrig  \\\n",
       "0         2              4         8         1         5         2        2   \n",
       "1         5              5         3         1        -2         2        1   \n",
       "2         5              3         4         1        -2         2        0   \n",
       "3         2              5         9         3         5         2        2   \n",
       "4         5              3         3         7        -2         2        2   \n",
       "5         2              6         8         1         5         1        2   \n",
       "6         2              2         5         1         5         3        1   \n",
       "7         5              7         4         3        -2         2        1   \n",
       "8         5              7         3         7        -2         2        1   \n",
       "9         5              5         4         4        -2         2        1   \n",
       "10        2              4         7         4         1         2        2   \n",
       "11        2              8         9         2         2         2        1   \n",
       "12        2              4         8         1         5         2        1   \n",
       "13        2              2         6         1         5         2        1   \n",
       "14        2              2         7         2         3         3        1   \n",
       "15        3              3         5         4         5         1        1   \n",
       "16        2              5         6         2         5         1        2   \n",
       "17        2              6         6         4         5         2        1   \n",
       "18        3              4         4         6         5         2        1   \n",
       "19        2              3         4         2         3         1        2   \n",
       "\n",
       "    equipm  acequipm_pub  totsqft_en   totalbtu  btubin  \n",
       "0        3             1        2100  144647.71       2  \n",
       "1        3             1         590   28034.61       0  \n",
       "2        2             1         900   30749.71       0  \n",
       "3        3             1        2100   86765.19       1  \n",
       "4        3             1         800   59126.93       1  \n",
       "5        3             1        4520   85400.64       1  \n",
       "6        3             1        2100  131875.03       2  \n",
       "7        4            -2         900   41446.59       0  \n",
       "8        5             4         750   14512.02       0  \n",
       "9        4             1         760   12393.76       0  \n",
       "10       3             1        1630   58496.25       1  \n",
       "11       7             1        1540   73867.11       1  \n",
       "12       3             1        2000   83979.36       1  \n",
       "13       2             4        1450   30595.98       0  \n",
       "14       3             4        1650   98033.63       2  \n",
       "15       5             1        1350   39937.69       0  \n",
       "16       2             6        1910  112775.20       2  \n",
       "17       2             6        2650  125582.35       2  \n",
       "18       3             1        1980   51945.31       0  \n",
       "19       4             1        1300   53803.14       0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display df\n",
    "df_housing.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into features and target variables\n",
    "X = df_housing.drop(['totalbtu', 'btubin'], axis=1)\n",
    "y = df_housing['btubin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing datasets by using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssays\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate the model\n",
    "LogR_Model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#fit the model\n",
    "LogR_Model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssays\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='saga')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate the model\n",
    "LogR_Model = LogisticRegression(solver='saga', max_iter=1000)\n",
    "\n",
    "#fit the model\n",
    "LogR_Model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssays\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='saga')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate the model\n",
    "LogR_Model = LogisticRegression(solver='saga', max_iter=1000, penalty='l2')\n",
    "\n",
    "#fit the model\n",
    "LogR_Model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate the model\n",
    "LogR_Model = LogisticRegression(solver='liblinear', max_iter=1000, penalty='l2')\n",
    "\n",
    "#fit the model\n",
    "LogR_Model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5789189189189189\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65      1232\n",
      "           1       0.47      0.35      0.40      1266\n",
      "           2       0.59      0.74      0.66      1202\n",
      "\n",
      "    accuracy                           0.58      3700\n",
      "   macro avg       0.57      0.58      0.57      3700\n",
      "weighted avg       0.57      0.58      0.57      3700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[806 290 136]\n",
      " [343 446 477]\n",
      " [ 90 222 890]]\n"
     ]
    }
   ],
   "source": [
    "#emake a prediction\n",
    "y_pred = LogR_Model.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "RFC_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RFC_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6143243243243244\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70      1232\n",
      "           1       0.50      0.45      0.47      1266\n",
      "           2       0.65      0.69      0.67      1202\n",
      "\n",
      "    accuracy                           0.61      3700\n",
      "   macro avg       0.61      0.62      0.61      3700\n",
      "weighted avg       0.61      0.61      0.61      3700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[873 272  87]\n",
      " [328 572 366]\n",
      " [ 64 310 828]]\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "y_pred =RFC_model.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 500, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_depth': 10}\n",
      "Best Score: 0.6416600065763636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform Random Search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=RFC_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_random = random_search.best_params_\n",
    "best_score_random = random_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params_random)\n",
    "print(\"Best Score:\", best_score_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.635945945945946\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.72      1232\n",
      "           1       0.51      0.49      0.50      1266\n",
      "           2       0.66      0.72      0.69      1202\n",
      "\n",
      "    accuracy                           0.64      3700\n",
      "   macro avg       0.64      0.64      0.64      3700\n",
      "weighted avg       0.63      0.64      0.63      3700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[866 290  76]\n",
      " [287 616 363]\n",
      " [ 37 294 871]]\n"
     ]
    }
   ],
   "source": [
    "#optimize\n",
    "RFC_optimized_model = RandomForestClassifier(n_estimators=400, min_samples_split=10, min_samples_leaf=6, max_features='sqrt', max_depth=10, random_state=42)\n",
    "RFC_optimized_model.fit(X_train, y_train)\n",
    "\n",
    "#evaluate\n",
    "y_pred =RFC_optimized_model.predict(X_test)\n",
    "opt_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", opt_accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Score: 0.6420654804855548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=RFC_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6424324324324324\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      1232\n",
      "           1       0.52      0.49      0.51      1266\n",
      "           2       0.67      0.73      0.70      1202\n",
      "\n",
      "    accuracy                           0.64      3700\n",
      "   macro avg       0.64      0.64      0.64      3700\n",
      "weighted avg       0.64      0.64      0.64      3700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[871 284  77]\n",
      " [290 623 353]\n",
      " [ 33 286 883]]\n"
     ]
    }
   ],
   "source": [
    "#optimize\n",
    "RFC_optimized_model = RandomForestClassifier(n_estimators=300, min_samples_split=2, min_samples_leaf=2, max_features='sqrt', max_depth=10, random_state=42)\n",
    "RFC_optimized_model.fit(X_train, y_train)\n",
    "\n",
    "#evaluate\n",
    "y_pred =RFC_optimized_model.predict(X_test)\n",
    "opt_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", opt_accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initiate model\n",
    "GBC_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "#train model\n",
    "GBC_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      1232\n",
      "           1       0.52      0.47      0.49      1266\n",
      "           2       0.67      0.74      0.70      1202\n",
      "\n",
      "    accuracy                           0.64      3700\n",
      "   macro avg       0.64      0.64      0.64      3700\n",
      "weighted avg       0.64      0.64      0.64      3700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[885 280  67]\n",
      " [296 596 374]\n",
      " [ 40 275 887]]\n"
     ]
    }
   ],
   "source": [
    "#make a prediction\n",
    "y_pred = GBC_model.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3875 - loss: 19.0765 - val_accuracy: 0.4668 - val_loss: 1.4475\n",
      "Epoch 2/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4648 - loss: 2.4054 - val_accuracy: 0.5335 - val_loss: 2.6055\n",
      "Epoch 3/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4812 - loss: 2.3508 - val_accuracy: 0.4611 - val_loss: 1.1911\n",
      "Epoch 4/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4736 - loss: 2.2735 - val_accuracy: 0.4951 - val_loss: 2.3410\n",
      "Epoch 5/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4829 - loss: 2.3250 - val_accuracy: 0.3327 - val_loss: 3.7878\n",
      "Epoch 6/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4949 - loss: 2.0689 - val_accuracy: 0.4386 - val_loss: 4.8129\n",
      "Epoch 7/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4888 - loss: 2.2464 - val_accuracy: 0.4614 - val_loss: 1.7997\n",
      "Epoch 8/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4901 - loss: 1.9968 - val_accuracy: 0.5054 - val_loss: 2.0558\n",
      "Epoch 9/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5155 - loss: 1.7311 - val_accuracy: 0.4378 - val_loss: 2.1760\n",
      "Epoch 10/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5050 - loss: 1.7907 - val_accuracy: 0.5535 - val_loss: 1.1326\n",
      "Epoch 11/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5010 - loss: 1.8235 - val_accuracy: 0.4676 - val_loss: 2.2802\n",
      "Epoch 12/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5033 - loss: 1.7211 - val_accuracy: 0.5876 - val_loss: 0.9087\n",
      "Epoch 13/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5170 - loss: 1.6046 - val_accuracy: 0.5238 - val_loss: 1.3034\n",
      "Epoch 14/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5255 - loss: 1.5058 - val_accuracy: 0.3389 - val_loss: 1.7798\n",
      "Epoch 15/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5147 - loss: 1.5263 - val_accuracy: 0.5276 - val_loss: 1.4506\n",
      "Epoch 16/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5196 - loss: 1.4931 - val_accuracy: 0.5059 - val_loss: 2.0053\n",
      "Epoch 17/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5093 - loss: 1.7128 - val_accuracy: 0.4681 - val_loss: 1.9636\n",
      "Epoch 18/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5077 - loss: 1.5576 - val_accuracy: 0.5441 - val_loss: 1.2070\n",
      "Epoch 19/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 1.4776 - val_accuracy: 0.5673 - val_loss: 1.0054\n",
      "Epoch 20/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5108 - loss: 1.4091 - val_accuracy: 0.5508 - val_loss: 1.0029\n",
      "Epoch 21/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5049 - loss: 1.4241 - val_accuracy: 0.5595 - val_loss: 1.0853\n",
      "Epoch 22/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5224 - loss: 1.3059 - val_accuracy: 0.4681 - val_loss: 2.2222\n",
      "Epoch 23/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4952 - loss: 1.7240 - val_accuracy: 0.5324 - val_loss: 1.2815\n",
      "Epoch 24/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5120 - loss: 1.5256 - val_accuracy: 0.4535 - val_loss: 1.5597\n",
      "Epoch 25/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5375 - loss: 1.2469 - val_accuracy: 0.4708 - val_loss: 1.2752\n",
      "Epoch 26/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5197 - loss: 1.2232 - val_accuracy: 0.4657 - val_loss: 1.6791\n",
      "Epoch 27/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5222 - loss: 1.2548 - val_accuracy: 0.5759 - val_loss: 0.8792\n",
      "Epoch 28/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 1.1644 - val_accuracy: 0.5624 - val_loss: 0.9663\n",
      "Epoch 29/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5421 - loss: 1.0471 - val_accuracy: 0.5430 - val_loss: 1.1266\n",
      "Epoch 30/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5304 - loss: 1.1936 - val_accuracy: 0.3381 - val_loss: 1.4984\n",
      "Epoch 31/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5269 - loss: 1.2349 - val_accuracy: 0.5389 - val_loss: 1.1589\n",
      "Epoch 32/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5343 - loss: 1.1356 - val_accuracy: 0.5862 - val_loss: 0.9050\n",
      "Epoch 33/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5332 - loss: 1.1706 - val_accuracy: 0.3470 - val_loss: 1.3711\n",
      "Epoch 34/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5266 - loss: 1.0817 - val_accuracy: 0.4541 - val_loss: 1.3386\n",
      "Epoch 35/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5346 - loss: 1.0959 - val_accuracy: 0.5797 - val_loss: 0.9257\n",
      "Epoch 36/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5378 - loss: 1.0486 - val_accuracy: 0.5941 - val_loss: 0.8541\n",
      "Epoch 37/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5483 - loss: 1.0279 - val_accuracy: 0.3697 - val_loss: 1.2306\n",
      "Epoch 38/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5350 - loss: 1.0561 - val_accuracy: 0.4776 - val_loss: 1.1176\n",
      "Epoch 39/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5417 - loss: 1.0635 - val_accuracy: 0.4605 - val_loss: 1.0969\n",
      "Epoch 40/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5446 - loss: 1.0549 - val_accuracy: 0.5884 - val_loss: 0.8988\n",
      "Epoch 41/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5534 - loss: 1.0264 - val_accuracy: 0.4762 - val_loss: 1.0566\n",
      "Epoch 42/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5334 - loss: 1.0423 - val_accuracy: 0.5373 - val_loss: 1.1031\n",
      "Epoch 43/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5471 - loss: 0.9893 - val_accuracy: 0.5262 - val_loss: 1.2502\n",
      "Epoch 44/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5403 - loss: 1.0220 - val_accuracy: 0.4932 - val_loss: 1.0025\n",
      "Epoch 45/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5339 - loss: 1.0019 - val_accuracy: 0.5524 - val_loss: 0.9081\n",
      "Epoch 46/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5652 - loss: 0.9371 - val_accuracy: 0.5538 - val_loss: 0.8991\n",
      "Epoch 47/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5479 - loss: 0.9719 - val_accuracy: 0.5689 - val_loss: 0.8776\n",
      "Epoch 48/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5549 - loss: 0.9509 - val_accuracy: 0.5495 - val_loss: 0.8870\n",
      "Epoch 49/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5566 - loss: 0.9306 - val_accuracy: 0.5943 - val_loss: 0.8515\n",
      "Epoch 50/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5648 - loss: 0.9570 - val_accuracy: 0.4841 - val_loss: 1.0858\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "nn_model = Sequential([\n",
    "    Input(shape=(X.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#compile model\n",
    "nn_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#convert target variable to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "#encode data\n",
    "y_train_encoded = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_encoded = to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "\n",
    "#train model\n",
    "history = nn_model.fit(X_train, y_train_encoded, epochs=50, validation_data=(X_val, y_val_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5468 - loss: 0.9644 - val_accuracy: 0.5903 - val_loss: 0.8482\n",
      "Epoch 2/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5567 - loss: 0.9402 - val_accuracy: 0.5773 - val_loss: 0.9227\n",
      "Epoch 3/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5584 - loss: 0.9485 - val_accuracy: 0.5646 - val_loss: 0.9323\n",
      "Epoch 4/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9818 - val_accuracy: 0.5727 - val_loss: 0.8695\n",
      "Epoch 5/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9224 - val_accuracy: 0.5441 - val_loss: 0.9369\n",
      "Epoch 6/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5645 - loss: 0.9191 - val_accuracy: 0.5614 - val_loss: 0.9586\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "#define model\n",
    "onn_model = Sequential([\n",
    "    Input(shape=(X.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#compile model\n",
    "onn_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#use early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "\n",
    "#convert target variable to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "#encode data\n",
    "y_train_encoded = to_categorical(y_train_encoded, num_classes=3)\n",
    "y_val_encoded = to_categorical(y_val_encoded, num_classes=3)\n",
    "\n",
    "#train model\n",
    "history = nn_model.fit(X_train, y_train_encoded, epochs=100, batch_size=32, validation_data=(X_val, y_val_encoded), callbacks=[early_stopping])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
